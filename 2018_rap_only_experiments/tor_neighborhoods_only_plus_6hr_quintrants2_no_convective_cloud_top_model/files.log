$ ruby make_grib_file_list.rb > tor_neighborhoods_only_plus_6hr_model/train.txt
Start year?
2014
Start month?
2
Start day?
26
End year?
2016
End month?
12
End day?
31
Forcast hour (2-18)?
6
Only hours with tornadoes (n/y)?
y
Dev ratio [0.0-1.0]?
0
Test ratio [0.0-1.0]?
0
Subsample ratio [0.0-1.0]? (non-deterministic)
1.0
1658/1658 hours with tornadoes

(need to manually remove headers)

$ ruby make_grib_file_list.rb
Start year?
2017
Start month?
1
Start day?
1
End year?
2017
End month?
12
End day?
31
Forcast hour (2-18)?
6
Only hours with tornadoes (n/y)?
y
Dev ratio [0.0-1.0]?
0.25
Test ratio [0.0-1.0]?
0.25
Subsample ratio [0.0-1.0]? (non-deterministic)
1.0

$ ruby make_grib_file_list.rb
Start year?
2017
Start month?
1
Start day?
1
End year?
2017
End month?
12
End day?
31
Forcast hour (2-18)?
6
Only hours with tornadoes (n/y)?
y
Dev ratio [0.0-1.0]?
0.25
Test ratio [0.0-1.0]?
0.25
Subsample ratio [0.0-1.0]? (non-deterministic)
1.0
# Dev Files
...

# Test Files
...

# Training Files
...

685/685 hours with tornadoes


(make dev.txt/test.txt files manually)
(add training files manually)

So training is 2014-2-26 through 2016 and half the tornado days in 2017.


# Take quintrants2 model and remove convective cloud top fields
# julia tor_neighborhoods_only_plus_6hr_quintrants2_no_convective_cloud_top_model/make_train_dev_binfeatures.jl


estimator =
  LightGBM.LGBMBinary(
    num_iterations = 1000,
    min_data_in_leaf = 1000, # 1000 did better than 2000 or 5000, 500 is too small
    # min_sum_hessian_in_leaf = 1000,
    learning_rate = .08, #
    early_stopping_round = 20,
    feature_fraction = .8,
    bagging_fraction = .8, #
    bagging_freq = 5,
    num_leaves = 25, # 15 is too small, 63 is too much; 40 - 0.066334
    is_sparse = false,
    max_bin = 255,
    num_threads = 4,
    is_unbalance = false, # if true, changes the label weights so pos/neg classes have same total weight across dataset when computing loss/gradient; causes WAAY too much probability all over
    metric = ["binary_logloss"]
  )

# dev loss 0.06659817866480855

$ ruby read_lightgbm_feature_importance.rb tor_neighborhoods_only_plus_6hr_quintrants2_no_convective_cloud_top_model/lightgbm_loss_0.0665982.model
67	LTNG:surface:56mi mean
48	PRATE:surface:56mi mean
41	LFTX:500-1000 mb:25mi mean
29	CAPE:90-0 mb above ground:25mi mean
26	RH:700 mb:56mi mean
24	HLCY:3000-0 m above ground:56mi mean
23	CAPE:90-0 mb above ground:25mi mean -1hr
22	CAPE:90-0 mb above ground:point -1hr
18	CAPE:90-0 mb above ground:56mi mean
18	VVEL:300 mb:56mi mean storm location -1hr
18	HLCY:3000-0 m above ground:56mi mean -1hr
17	PRES:80 m above ground:56mi mean storm location -1hr
15	LFTX:500-1000 mb:25mi mean storm location -1hr
15	CAPE:90-0 mb above ground:25mi mean storm location -1hr
15	CAPE:90-0 mb above ground:point
15	RH:700 mb:25mi-56mi leftward gradient
14	PRATE:surface:56mi mean storm location -1hr
13	HLCY:1000-0 m above ground:56mi mean
13	4LFTX:180-0 mb above ground:25mi mean
12	RGRD:speed m/s 650 mb:25mi mean
12	4LFTX:180-0 mb above ground:25mi mean storm location -1hr
12	4LFTX:180-0 mb above ground:56mi mean storm location -1hr
12	PLPL:255-0 mb above ground:56mi mean
11	RH:300 mb:56mi mean -1hr
11	HGT:700 mb:25mi-56mi forward gradient
11	PRES:max wind:25mi mean -1hr
11	RH:450 mb:56mi mean -1hr
11	HLCY:3000-0 m above ground:25mi mean -1hr
10	TMP:350 mb:25mi-56mi leftward gradient
10	HLCY:1000-0 m above ground:25mi mean -1hr




estimator =
  LightGBM.LGBMBinary(
    num_iterations = 1000,
    min_data_in_leaf = 1000, # 1000 did better than 2000 or 5000, 500 is too small
    # min_sum_hessian_in_leaf = 1000,
    learning_rate = .08, #
    early_stopping_round = 20,
    feature_fraction = .8, # .9 – 0.06654782621229002; .8 – 0.06607885426312127; .7 — 0.06638090709783184
    bagging_fraction = .8, #
    bagging_freq = 5,
    num_leaves = 27, # 15 is too small, 63 is too much; 40 - 0.066334
    is_sparse = false,
    max_bin = 255,
    num_threads = 4,
    is_unbalance = false, # if true, changes the label weights so pos/neg classes have same total weight across dataset when computing loss/gradient; causes WAAY too much probability all over
    metric = ["binary_logloss"]
  )

# dev loss 0.06650738412755652

estimator =
  LightGBM.LGBMBinary(
    num_iterations = 1000,
    min_data_in_leaf = 1200, # 1000 did better than 2000 or 5000, 500 is too small
    # min_sum_hessian_in_leaf = 1000,
    learning_rate = .08, #
    early_stopping_round = 20,
    feature_fraction = .8, # .9 – 0.06654782621229002; .8 – 0.06607885426312127; .7 — 0.06638090709783184
    bagging_fraction = .8, #
    bagging_freq = 5,
    num_leaves = 29, # 15 is too small, 63 is too much; 40 - 0.066334
    is_sparse = false,
    max_bin = 255,
    num_threads = 4,
    is_unbalance = false, # if true, changes the label weights so pos/neg classes have same total weight across dataset when computing loss/gradient; causes WAAY too much probability all over
    metric = ["binary_logloss"]
  )

# dev loss 0.066664630964556

estimator =
  LightGBM.LGBMBinary(
    num_iterations = 1000,
    min_data_in_leaf = 900, # 1000 did better than 2000 or 5000, 500 is too small
    # min_sum_hessian_in_leaf = 1000,
    learning_rate = .08, #
    early_stopping_round = 20,
    feature_fraction = .8, # .9 – 0.06654782621229002; .8 – 0.06607885426312127; .7 — 0.06638090709783184
    bagging_fraction = .8, #
    bagging_freq = 5,
    num_leaves = 29, # 15 is too small, 63 is too much; 40 - 0.066334
    is_sparse = false,
    max_bin = 255,
    num_threads = 4,
    is_unbalance = false, # if true, changes the label weights so pos/neg classes have same total weight across dataset when computing loss/gradient; causes WAAY too much probability all over
    metric = ["binary_logloss"]
  )

# dev loss 0.06631661915347456
