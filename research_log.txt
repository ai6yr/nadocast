- Figured out distance and wrote distance_and_midpoint, waypoints, distance_to_line functions in Julia using GeographicLib exposed by Proj4
- Downloaded and inspected schema changes in +1hr inventory files. (Downloaded by modifying get_rap.rb; run ruby examine_inventory.rb to dump files where schema changed to inventory_changes/ as well as print out a summary of common layers to stdout.)
    - Apparently, simulated reflectivity wasn't added to RUC until late 2008
- Downloaded US national boundary shapefile from https://www.nohrsc.noaa.gov/gisdatasets/ (ln_us/ln_us.shp). Apparently this file is CONUS-only, which is what we want.
- Also obtained states shapefile from https://www.weather.gov/gis/USStates for possible later use (s_11au16/s_11au16.shp). https://www.weather.gov/gis/USStates has explanation of layers
- Installed wgrib2 according to https://bovineaerospace.wordpress.com/2017/08/20/how-to-install-wgrib2-in-osx/ except I used "export CC=clang" with "export FC=gfortran". Symlinked to the built binary /Users/brian/Documents/open_source/grib2/wgrib2/wgrib2
- Imported grid (make grid_coords_only.csv) into GRASS GIS (symlinked so I can start with $ grassgis) along with the CONUS shapefile and instructed it to find the intersection (see grass_gis_grid_conus_intersection.png for settings)
    - Output file is "grid_xys_inside_conus.txt"; columns are lon,lat,i (not lat lon!) where i is the line number (starting from 1) in grid_coords_only.csv and grid.csv
    - "conus_grid_gis.gxw" is just saved workspace settings; refers to layers stored in a GIS workspace elsewhere on my computer
- Downloaded all +1 hour RUC/RAP forecasts between 2008-11-18 00:00 and 2018-02-15 23:00 to 8TB external disk. 1.4TB used. (Includes previously obtained +2 hour forecasts for 2017-01-01 through 2017-09-30)
- `wgrib2 -bin` binary output is explained here: http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/bin_ieee_text_format.html Checking the wgrib2 source (File.c) floats and ints are 4 bytes each (or rather whatever sizeof(float) and sizeof(int) was on compilation). Note there's a "header" int before AND after each grid.
    - `wgrib2 -help asdf` seems to be equivalent to `wgrib2 | grep asdf`
    - See also long list of command line options here: http://www.cpc.ncep.noaa.gov/products/wesley/wgrib2/long_cmd_list.html
    - `wgrib2 asdf.grb -header -bin -`
- read_grib.jl
    - read grib for inventory, normalize and select fields, read those out in binary
    - convert UV winds to polar r/theta
    - rotate thetas relative to storm direction
    - tell wgrib2 to rotate the storm winds to lat/lon aligned (changes velocities by <1%; changes to storm angle of course more dramatic)

- cut out grid points within 25 miles of edge of CONUS (expanded conus edge by 1 mile using MMQGIS plugin, polygon->polyline, made 26 mile buffer using MMQGIS. Removed CONUS points inside that buffer zone. CSV export seems not to honor filtering, but using GRASS v.select from inside QGIS is successful in actually removing the hidden points.)
- estimated area represented by each gridpoint for later use when weighting observations (for NN training, use weight as probability of use)

- TODO check whether order of grid points in grid.csv is same as order of data output in binary format



- More convective fields were added to the RAP at 2014-02-25 12UTC, lets focus on dates after that.

- baseline loss (computed correctly) is 0.079 training 0.081 dev using a constant prediction. (+1 hour, 250mi neighborhoods)
- Most saved +1hour models have bad dev loss numbers on them (notable exception of LightGBM models)
- Multiplier for bad dev loss to correct for when denominator was point count instead of weight: 1.06059
- But retesting "4factor_logistic" reveals its dev loss was actually 0.06038, so LightGBM does do better (although LightGBM doesn't weight the dev samples :( )


- Tried tons of ML algorithms on +1 forecasts and added lots of features (see headers.txt). LightGBM performs best, but training data really can't be more than 40GB on my machine.
- Probably limited by precision of the RAP, but we'll see.


-- 2019 --

Going to train separate models HREF, SREF, HRRR (hopefully), and RAP and then stack them together. Would like to try convnets, but unlikely to have the computational power needed for training a convnet of sufficient complexity to make it worthwhile.

Also would like not to be memory limited for training boosted decision trees. May have to find a streaming implementation or write my own.

Also going to try to use Julia notebooks to see if that can reduce frustration.

- Flux.Conv() convolutional layer constructor does not work with Float32 arrays (could hack around if needed: problem seems to be that the constructors make Float64 weights in the model...so wierd in any case)

Just discovered that the SREF "1hrly" forecasts only contain the hours not divisible by three (1,2,4,5,7,8 etc). Going to start to grab the "3hrly" forecasts as well. So 2019-01-09 onward will have 1hrly (out to 38 horus) and 3hrly (out to 87 hours).

- [ ] Update SREF.jl

Why is logistic regression not learning on SREF...(consistant 0.3 validation loss)?

Okay got it to learn. Need enough variety of data and an appropriate starting learning rate (neither too high or low).

Also switched to 64bit math to (slightly) reduce likelihood of exploding gradient. May be able to switch back but there's no reason to until we start hitting memory problems. The slow part is wgrib2 data loading ().


TODO:

- [x] Rename MagicTreeBoosting to something pushable to Github.

- [x] Upgrade to Julia 1.1.0

- [x] Not briefly require double memory during data loading (Push features to disk while loading)

- [x] Try using row-major memory layout data for speed (might be fewer cache misses during histogram building).
        It's 3x slower and I'm not sure I can speed it up enough.

- [x] Speed up feature engineering by pulling different portions into functions. Small functions sped up the tree booster.

- [x] Switch to 5/1/1 (weekdays/sat/sun) for training/validation/test
- [ ] Use storm reports DB if ready for 2018
- [ ] Add Jan/Feb/March 2019 live storm reports
- [ ] Use all all severe report hours (wind/hail/tornado) for train/validation/test
- [ ] Write the "annealing" for data subsetting/weighting
- [ ] Train +2hr RAP
- [ ] Train +6hr RAP
- [ ] Train +12hr RAP
- [ ] Train +18hr RAP
- [ ] Train +2hr HRRR
- [ ] Train +6hr HRRR
- [ ] Train +12hr HRRR
- [ ] Train +18hr HRRR
- [ ] Train SREF
- [ ] Train HREF
- [ ] Try training 1-6, 7-12, 13-18, 19-24, 25-30, 31-39; or 1-6, 7-18, 19-39 hour HREF/SREF sets
- [ ] Train a stacked model
- [ ] Try training 1-6, 7-12, 13-18, 19-24, 25-30, 31-39; or 1-6, 7-18, 19-39 hour stacked models
- [ ] Calibrate on 20ish random validation days (Saturdays)

- [ ] Build dataset for daily probs
- [ ] Train it
- [ ] Calibrate it
